defaults: 
  - model_family
  - _self_

data_path: data/safety_bench/HEx-PHI.jsonl 
# data_path: sorry-bench/sorry-bench-202503
# data_path: walledai/AdvBench

results_path: outputs/safety_outputs/

batch_size: 16 # 
# model_path: Qwen/Qwen2.5-32B-Instruct
model_path: /scratch/refusal-unlearning/refusal_qwen2_7b

# model generation on each unsafe prompt
model_gen_kwargs:
  max_new_tokens: 512
  do_sample: False
  # top_p: 0.6
  temperature: 0.3
  top_k: 50
  repetition_penalty: 1.0

eval_template: plain
prefill_prefix: 
num_prefix_tokens: 0

# safety related parameters
evaluator: llama_guard # [none, key_word, llama_guard, mistral, gpt]
safe_eval_only: False  # if True, only do safety evaluation

# arguments for fine-tuning, not for evaluation!
dataset: 
  refusal_path: data/ft/alpaca_gpt4_refusal.json # Local path to the refusal dataset
  # refusal_path: data/ft/alpaca_gpt4_random.json # Local path to the refusal dataset
  hf_path: vicgalle/alpaca-gpt4 # HuggingFace dataset
  benign_path: data/ft/alpaca_gpt4_benign.json 
  ratio: 1.0 # Ratio of the dataset to use for training